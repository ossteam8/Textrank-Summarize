{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "narrative-motivation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Komoran\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "import kss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-graduate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "comparable-floating",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "silver-warehouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = \"\"\" 국내 신종 코로나바이러스 감염증(코로나19) 신규 확진자가 하루만에 600명대로 올라섰다.전국적으로 크고 작은 일상 감염이 잇따르는 상황에서 해외유입 변이 바이러스도 빠르게 확산되고 있어 당국이 촉각을 세우고 있다. 정부는 오는 21일 다음주부터 적용할 사회적 거리두기 조정안을 발표할 예정이다.19일 중앙방역대책본부(중대본)에 따르면 이날 0시 기준 코로나19 신규 확진자는 654명으로 전날(528명)보다 126명 늘었다.누적 확진자는 13만3471명이다.확진자 수는 보통 주말·휴일 검사건수 감소 영향으로 인해 주 초반에는 비교적 적게 나오다가 중반부터 증가하는 경향을 보인다.이날 신규 확진자의 감염경로는 지역발생이 637명, 해외유입이 17명이다.지역별로는 서울 245명, 경기 159명, 인천 23명 등 수도권이 427명(67%)이다.주요 집단발병 사례를 보면 노래연습장, 유흥업소 등 다중이용시설을 고리로 새로운 감염이 확인됐다.수도권에서는 서울 강동구 노래연습장과 관련해 41명, 서울 노원구 고시원에서 11명, 경기 성남시 일가족-지인 사례에서 19명이 각각 양성 판정을 받았다.앞서 발생한 인천국제공항 검역소 집단감염과 관련해선 8명이 인도 변이에 감염된 것으로 확인됐다. 역학적 관련성이 있는 나머지 7명까지 포함하면 15명 전원이 인도 변이 감염자인 셈이다.최근 코로나19 발생 양상을 보면 전국에서 집단감염이 잇따르면서 '4차 유행'이 지속하고 있다.지난 13일부터 이날까지 최근 1주일간 하루 평균 신규 확진자는 약 651명꼴이다. 이중 '사회적 거리두기' 단계 조정의 핵심 지표인 일평균 지역발생 확진자는 약 628명으로, 여전히 2.5단계(전국 400∼500명 이상 등) 범위다.사망자는 전날보다 8명 늘어 누적 1912명이 됐다. 국내 평균 치명률은 1.43%다.전날 하루 선별진료소를 통한 검사 건수는 3만3640건으로, 직전일(4만1704건)보다 19% 적다.검사건수 대비 확진자를 계산한 양성률은 1.94%(3만3640명 중 654명)로, 직전일 1.27%(4만1704명 중 528명)보다 상승했다.정부는 오는 21일 다음주부터 3주간 적용할 사회적 거리두기 조정안을 발표한다. 현재 수도권에서는 2단계, 비수도권에서는 1.5단계의 거리두기 조처가 이뤄지고 있다.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-andrew",
   "metadata": {},
   "outputs": [],
   "source": [
    "news1 = \"\"\" 국내 신종 코로나바이러스 감염증(코로나19) 신규 확진자가 하루만에 600명대로 올라섰다.전국적으로 크고 작은 일상 감염이 잇따르는 상황에서 해외유입 변이 바이러스도 빠르게 확산되고 있어 당국이 촉각을 세우고 있다. 정부는 오는 21일 다음주부터 적용할 사회적 거리두기 조정안을 발표할 예정이다.19일 중앙방역대책본부(중대본)에 따르면 이날 0시 기준 코로나19 신규 확진자는 654명으로 전날(528명)보다 126명 늘었다.누적 확진자는 13만3471명이다.확진자 수는 보통 주말·휴일 검사건수 감소 영향으로 인해 주 초반에는 비교적 적게 나오다가 중반부터 증가하는 경향을 보인다.이날 신규 확진자의 감염경로는 지역발생이 637명, 해외유입이 17명이다.지역별로는 서울 245명, 경기 159명, 인천 23명 등 수도권이 427명(67%)이다.주요 집단발병 사례를 보면 노래연습장, 유흥업소 등 다중이용시설을 고리로 새로운 감염이 확인됐다.수도권에서는 서울 강동구 노래연습장과 관련해 41명, 서울 노원구 고시원에서 11명, 경기 성남시 일가족-지인 사례에서 19명이 각각 양성 판정을 받았다.앞서 발생한 인천국제공항 검역소 집단감염과 관련해선 8명이 인도 변이에 감염된 것으로 확인됐다. 역학적 관련성이 있는 나머지 7명까지 포함하면 15명 전원이 인도 변이 감염자인 셈이다.최근 코로나19 발생 양상을 보면 전국에서 집단감염이 잇따르면서 '4차 유행'이 지속하고 있다.지난 13일부터 이날까지 최근 1주일간 하루 평균 신규 확진자는 약 651명꼴이다. 이중 '사회적 거리두기' 단계 조정의 핵심 지표인 일평균 지역발생 확진자는 약 628명으로, 여전히 2.5단계(전국 400∼500명 이상 등) 범위다.사망자는 전날보다 8명 늘어 누적 1912명이 됐다. 국내 평균 치명률은 1.43%다.전날 하루 선별진료소를 통한 검사 건수는 3만3640건으로, 직전일(4만1704건)보다 19% 적다.검사건수 대비 확진자를 계산한 양성률은 1.94%(3만3640명 중 654명)로, 직전일 1.27%(4만1704명 중 528명)보다 상승했다.정부는 오는 21일 다음주부터 3주간 적용할 사회적 거리두기 조정안을 발표한다. 현재 수도권에서는 2단계, 비수도권에서는 1.5단계의 거리두기 조처가 이뤄지고 있다.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defensive-compatibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "news2 = \"\"\"경북 영천에서 17일 하루 만에 신종 코로나바이러스 감염증(코로나19) 확진자가 5명이 발생, N차 감염이 우려되는 가운데 비상이 걸렸다. 지난 7일 80번 확진자 이후 열흘 만에 지역 누적 확진자는 85명으로 늘어났다.영천시에 따르면 경산에 주소를 둔 공부방 교사가 확진됐다는 소식을 접한 A여고 한 학생이 지난 16일 보건소 검체 검사 결과 17일 양성판정을 받았다.또 확진자 교사가 운영 중인 공부방에서 과외 학습을 받고 있는 B여고 학생이 17일 오후 8시 확진 판정을 받은 가운데 A여고 학생 부모와 B여고 학생 엄마 등 총 5명이 코로나 판정을 받았다.수업을 받은 나머지 학생들은 모두 음성 판정을 받은 것으로 알려졌다.이날 A여고 측은 학생, 교직원 등 140여 명이 보건소에서 검체를 실시했으며 뒤늦게 알려진 B여고는 18일 학생과 교직원들이 검사를 받을 예정이다.이에 보건소는 코로나 확진자들을 자가 격리하고 이들의 이동 동선 파악에 집중하는 가운데 학교 등의 방역소독을 철저히 하고 있다.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "round-partner",
   "metadata": {},
   "outputs": [],
   "source": [
    "news3 =\"\"\" 브라질, 인도발 코로나 변이 상륙할라…긴장 속 모니터링 강화 (상파울루=연합뉴스) 김재순 특파원 = 브라질 보건당국이 인도발 신종 코로나바이러스 감염증(코로나19) 변이 바이러스 상륙 가능성에 긴장하고 있다.18일(현지시간) 브라질 언론에 따르면 상파울루시 당국은 인도발 변이 바이러스가 브라질에서도 나타날 가능성이 크다는 지적에 따라 상파울루주 정부 산하 부탄탕연구소와 함께 모니터링을 강화하기로 했다고 밝혔다.상파울루시는 브라질에서 코로나19 확진자와 사망자가 가장 먼저 나온 곳으로, 인근 과룰류스 국제공항을 통해 외국인 입국이 대규모로 이뤄지고 있다.상파울루시 관계자는 이날 기자회견을 통해 \"인도발 변이 바이러스 상륙을 막기 위한 사전 조치로 부탄탕연구소와 협력해 외국인에 대한 철저한 추적 관찰이 이뤄질 것\"이라고 말했다.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "juvenile-assistant",
   "metadata": {},
   "outputs": [],
   "source": [
    "news4 = \"\"\" 코로나 방역 모범국으로 평가받던 대만, 베트남, 싱가포르에서 최근 코로나 확진자가 크게 늘면서 방역에 비상이 걸렸다. 결국 백신 접종이 이뤄지지 않으면 코로나 확산세를 억제하기는 어렵다는 지적이 나온다. 미 블룸버그통신은 대만과 싱가포르가 자국내 방역 단계를 빠르게 격상시키고 있다고 16일 보도했다. 대만 정부는 이번 주말에 시민들이 집에서 머무르도록 권고하고, 실내 모임은 최대 5명, 야외 모임은 10명까지 제한했다. 국제 통계사이트 월드오미터 기준 대만에서 16일 보고된 코로나 신규 확진자는 207명으로 최고치를 기록했다.싱가포르 역시 식당 내에서의 식사를 금지하고 재택근무를 의무화했으며, 사적 모임은 최대 2인까지로 제한했다. 싱가포르 정부가 이런 봉쇄 정책을 실시한 것은 지난해 4~5월 이후 1년 만이다. 16일 보고된 싱가포르 신규 확진자는 49명이지만, 5월 초부터 집단 감염이 발생하자 경계 수위를 높였다. 또 최근 21일간 대만 여행 이력이 있는 외국인의 입국을 금지했다.대만과 싱가포르는 지난 12월 양국 간의 여행 제한 조치를 완화했었다. 양국에서 서로 체류하다 온 이들에 대해 자가격리 절차를 면제해 주기로 한 것이다. 하지만 자국 내 코로나 확산세가 심해지면서 여행객들에 대한 빗장도 다시 걸어 잠갔다.베트남은 이날 월드오미터 기준 127명의 신규 확진자가 보고됐다. 그 전날에는 165명의 확진자가 보고됐는데, 이는 지난해 1월 베트남에서 코로나가 처음 발병한 이래 최고치다. 북부 바짱주의 꽝차우 산업단지에서 집단감염이 일어나면서 확진자 수가 치솟았다.블룸버그는 이런 현상에 대해 “지역 사회에 대한 백신 접종 없이 바이러스로부터 안전한 환경을 만드는 게 얼마나 어려운지를 보여주는 사례”라고 했다. 블룸버그에 따르면 대만의 백신 접종률은 1% 미만이고, 베트남 역시 백신 접종률은 인구의 1% 수준이다. 싱가포르는 발빠르게 백신 확보에 나선 나라 중 하나이지만, 지금까지 한번이라도 백신을 맞은 국민은 전체 인구의 3분의 1 정도다.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "illegal-assault",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "desperate-degree",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['국내 신종 코로나바이러스 감염증(코로나19) 신규 확진자가 하루만에 600명대로 올라섰다.', '전국적으로 크고 작은 일상 감염이 잇따르는 상황에서 해외유입 변이 바이러스도 빠르게 확산되고 있어 당국이 촉각을 세우고 있다.', '정부는 오는 21일 다음주부터 적용할 사회적 거리두기 조정안을 발표할 예정이다.', '19일 중앙방역대책본부(중대본)에 따르면 이날 0시 기준 코로나19 신규 확진자는 654명으로 전날(528명)보다 126명 늘었다.', '누적 확진자는 13만3471명이다.', '확진자 수는 보통 주말·휴일 검사건수 감소 영향으로 인해 주 초반에는 비교적 적게 나오다가 중반부터 증가하는 경향을 보인다.', '이날 신규 확진자의 감염경로는 지역발생이 637명, 해외유입이 17명이다.', '지역별로는 서울 245명, 경기 159명, 인천 23명 등 수도권이 427명(67%)이다.', '주요 집단발병 사례를 보면 노래연습장, 유흥업소 등 다중이용시설을 고리로 새로운 감염이 확인됐다.', '수도권에서는 서울 강동구 노래연습장과 관련해 41명, 서울 노원구 고시원에서 11명, 경기 성남시 일가족-지인 사례에서 19명이 각각 양성 판정을 받았다.', '앞서 발생한 인천국제공항 검역소 집단감염과 관련해선 8명이 인도 변이에 감염된 것으로 확인됐다.', '역학적 관련성이 있는 나머지 7명까지 포함하면 15명 전원이 인도 변이 감염자인 셈이다.', \"최근 코로나19 발생 양상을 보면 전국에서 집단감염이 잇따르면서 '4차 유행'이 지속하고 있다.\", '지난 13일부터 이날까지 최근 1주일간 하루 평균 신규 확진자는 약 651명꼴이다.', \"이중 '사회적 거리두기' 단계 조정의 핵심 지표인 일평균 지역발생 확진자는 약 628명으로, 여전히 2.5단계(전국 400∼500명 이상 등) 범위다.\", '사망자는 전날보다 8명 늘어 누적 1912명이 됐다.', '국내 평균 치명률은 1.43%다.', '전날 하루 선별진료소를 통한 검사 건수는 3만3640건으로, 직전일(4만1704건)보다 19% 적다.', '검사건수 대비 확진자를 계산한 양성률은 1.94%(3만3640명 중 654명)로, 직전일 1.27%(4만1704명 중 528명)보다 상승했다.', '정부는 오는 21일 다음주부터 3주간 적용할 사회적 거리두기 조정안을 발표한다.', '현재 수도권에서는 2단계, 비수도권에서는 1.5단계의 거리두기 조처가 이뤄지고 있다.']\n"
     ]
    }
   ],
   "source": [
    "text_list = kss.split_sentences(sents)\n",
    "print(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continent-certificate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "reliable-cover",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def scan_vocabulary(sents, tokenize=None, min_count=2):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    sents : list of str\n",
    "        Sentence list\n",
    "    tokenize : callable\n",
    "        tokenize(str) returns list of str\n",
    "    min_count : int\n",
    "        Minumum term frequency\n",
    "    Returns\n",
    "    -------\n",
    "    idx_to_vocab : list of str\n",
    "        Vocabulary list\n",
    "    vocab_to_idx : dict\n",
    "        Vocabulary to index mapper.\n",
    "    \"\"\"\n",
    "    counter = Counter(w for sent in sents for w in tokenize(sent))\n",
    "    counter = {w:c for w,c in counter.items() if c >= min_count}\n",
    "    idx_to_vocab = [w for w, _ in sorted(counter.items(), key=lambda x:-x[1])]\n",
    "    vocab_to_idx = {vocab:idx for idx, vocab in enumerate(idx_to_vocab)}\n",
    "    return idx_to_vocab, vocab_to_idx\n",
    "\n",
    "def tokenize_sents(sents, tokenize):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    sents : list of str\n",
    "        Sentence list\n",
    "    tokenize : callable\n",
    "        tokenize(sent) returns list of str (word sequence)\n",
    "    Returns\n",
    "    -------\n",
    "    tokenized sentence list : list of list of str\n",
    "    \"\"\"\n",
    "    return [tokenize(sent) for sent in sents]\n",
    "\n",
    "def vectorize(tokens, vocab_to_idx):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    tokens : list of list of str\n",
    "        Tokenzed sentence list\n",
    "    vocab_to_idx : dict\n",
    "        Vocabulary to index mapper\n",
    "    Returns\n",
    "    -------\n",
    "    sentence bow : scipy.sparse.csr_matrix\n",
    "        shape = (n_sents, n_terms)\n",
    "    \"\"\"\n",
    "    rows, cols, data = [], [], []\n",
    "    for i, tokens_i in enumerate(tokens):\n",
    "        for t, c in Counter(tokens_i).items():\n",
    "            j = vocab_to_idx.get(t, -1)\n",
    "            if j == -1:\n",
    "                continue\n",
    "            rows.append(i)\n",
    "            cols.append(j)\n",
    "            data.append(c)\n",
    "    n_sents = len(tokens)\n",
    "    n_terms = len(vocab_to_idx)\n",
    "    x = csr_matrix((data, (rows, cols)), shape=(n_sents, n_terms))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agreed-document",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "honey-member",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "\n",
    "\n",
    "def sent_graph(sents, tokenize=None, min_count=2, min_sim=0.3,\n",
    "    similarity=None, vocab_to_idx=None, verbose=False):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    sents : list of str\n",
    "        Sentence list\n",
    "    tokenize : callable\n",
    "        tokenize(sent) return list of str\n",
    "    min_count : int\n",
    "        Minimum term frequency\n",
    "    min_sim : float\n",
    "        Minimum similarity between sentences\n",
    "    similarity : callable or str\n",
    "        similarity(s1, s2) returns float\n",
    "        s1 and s2 are list of str.\n",
    "        available similarity = [callable, 'cosine', 'textrank']\n",
    "    vocab_to_idx : dict\n",
    "        Vocabulary to index mapper.\n",
    "        If None, this function scan vocabulary first.\n",
    "    verbose : Boolean\n",
    "        If True, verbose mode on\n",
    "    Returns\n",
    "    -------\n",
    "    sentence similarity graph : scipy.sparse.csr_matrix\n",
    "        shape = (n sents, n sents)\n",
    "    \"\"\"\n",
    "\n",
    "    if vocab_to_idx is None:\n",
    "        idx_to_vocab, vocab_to_idx = scan_vocabulary(sents, tokenize, min_count)\n",
    "    else:\n",
    "        idx_to_vocab = [vocab for vocab, _ in sorted(vocab_to_idx.items(), key=lambda x:x[1])]\n",
    "\n",
    "    x = vectorize_sents(sents, tokenize, vocab_to_idx)\n",
    "    if similarity == 'cosine':\n",
    "        x = numpy_cosine_similarity_matrix(x, min_sim, verbose, batch_size=1000)\n",
    "    else:\n",
    "        x = numpy_textrank_similarity_matrix(x, min_sim, verbose, batch_size=1000)\n",
    "    return x\n",
    "\n",
    "def vectorize_sents(sents, tokenize, vocab_to_idx):\n",
    "    rows, cols, data = [], [], []\n",
    "    for i, sent in enumerate(sents):\n",
    "        counter = Counter(tokenize(sent))\n",
    "        for token, count in counter.items():\n",
    "            j = vocab_to_idx.get(token, -1)\n",
    "            if j == -1:\n",
    "                continue\n",
    "            rows.append(i)\n",
    "            cols.append(j)\n",
    "            data.append(count)\n",
    "    n_rows = len(sents)\n",
    "    n_cols = len(vocab_to_idx)\n",
    "    return csr_matrix((data, (rows, cols)), shape=(n_rows, n_cols))\n",
    "\n",
    "def numpy_cosine_similarity_matrix(x, min_sim=0.3, verbose=True, batch_size=1000):\n",
    "    n_rows = x.shape[0]\n",
    "    mat = []\n",
    "    for bidx in range(math.ceil(n_rows / batch_size)):\n",
    "        b = int(bidx * batch_size)\n",
    "        e = min(n_rows, int((bidx+1) * batch_size))\n",
    "        psim = 1 - pairwise_distances(x[b:e], x, metric='cosine')\n",
    "        rows, cols = np.where(psim >= min_sim)\n",
    "        data = psim[rows, cols]\n",
    "        mat.append(csr_matrix((data, (rows, cols)), shape=(e-b, n_rows)))\n",
    "        if verbose:\n",
    "            print('\\rcalculating cosine sentence similarity {} / {}'.format(b, n_rows), end='')\n",
    "    mat = sp.sparse.vstack(mat)\n",
    "    if verbose:\n",
    "        print('\\rcalculating cosine sentence similarity was done with {} sents'.format(n_rows))\n",
    "    return mat\n",
    "\n",
    "def numpy_textrank_similarity_matrix(x, min_sim=0.3, verbose=True, min_length=1, batch_size=1000):\n",
    "    n_rows, n_cols = x.shape\n",
    "\n",
    "    # Boolean matrix\n",
    "    rows, cols = x.nonzero()\n",
    "    data = np.ones(rows.shape[0])\n",
    "    z = csr_matrix((data, (rows, cols)), shape=(n_rows, n_cols))\n",
    "\n",
    "    # Inverse sentence length\n",
    "    size = np.asarray(x.sum(axis=1)).reshape(-1)\n",
    "    size[np.where(size <= min_length)] = 10000\n",
    "    size = np.log(size)\n",
    "\n",
    "    mat = []\n",
    "    for bidx in range(math.ceil(n_rows / batch_size)):\n",
    "\n",
    "        # slicing\n",
    "        b = int(bidx * batch_size)\n",
    "        e = min(n_rows, int((bidx+1) * batch_size))\n",
    "\n",
    "        # dot product\n",
    "        inner = z[b:e,:] * z.transpose()\n",
    "\n",
    "        # sentence len[i,j] = size[i] + size[j]\n",
    "        norm = size[b:e].reshape(-1,1) + size.reshape(1,-1)\n",
    "        norm = norm ** (-1)\n",
    "        norm[np.where(norm == np.inf)] = 0\n",
    "\n",
    "        # normalize\n",
    "        sim = inner.multiply(norm).tocsr()\n",
    "        rows, cols = (sim >= min_sim).nonzero()\n",
    "        data = np.asarray(sim[rows, cols]).reshape(-1)\n",
    "\n",
    "        # append\n",
    "        mat.append(csr_matrix((data, (rows, cols)), shape=(e-b, n_rows)))\n",
    "\n",
    "        if verbose:\n",
    "            print('\\rcalculating textrank sentence similarity {} / {}'.format(b, n_rows), end='')\n",
    "\n",
    "    mat = sp.sparse.vstack(mat)\n",
    "    if verbose:\n",
    "        print('\\rcalculating textrank sentence similarity was done with {} sents'.format(n_rows))\n",
    "\n",
    "    return mat\n",
    "\n",
    "def graph_with_python_sim(tokens, verbose, similarity, min_sim):\n",
    "    if similarity == 'cosine':\n",
    "        similarity = cosine_sent_sim\n",
    "    elif callable(similarity):\n",
    "        similarity = similarity\n",
    "    else:\n",
    "        similarity = textrank_sent_sim\n",
    "\n",
    "    rows, cols, data = [], [], []\n",
    "    n_sents = len(tokens)\n",
    "    for i, tokens_i in enumerate(tokens):\n",
    "        if verbose and i % 1000 == 0:\n",
    "            print('\\rconstructing sentence graph {} / {} ...'.format(i, n_sents), end='')\n",
    "        for j, tokens_j in enumerate(tokens):\n",
    "            if i >= j:\n",
    "                continue\n",
    "            sim = similarity(tokens_i, tokens_j)\n",
    "            if sim < min_sim:\n",
    "                continue\n",
    "            rows.append(i)\n",
    "            cols.append(j)\n",
    "            data.append(sim)\n",
    "    if verbose:\n",
    "        print('\\rconstructing sentence graph was constructed from {} sents'.format(n_sents))\n",
    "    return csr_matrix((data, (rows, cols)), shape=(n_sents, n_sents))\n",
    "\n",
    "def textrank_sent_sim(s1, s2):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    s1, s2 : list of str\n",
    "        Tokenized sentences\n",
    "    Returns\n",
    "    -------\n",
    "    Sentence similarity : float\n",
    "        Non-negative number\n",
    "    \"\"\"\n",
    "    n1 = len(s1)\n",
    "    n2 = len(s2)\n",
    "    if (n1 <= 1) or (n2 <= 1):\n",
    "        return 0\n",
    "    common = len(set(s1).intersection(set(s2)))\n",
    "    base = math.log(n1) + math.log(n2)\n",
    "    return common / base\n",
    "\n",
    "def cosine_sent_sim(s1, s2):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    s1, s2 : list of str\n",
    "        Tokenized sentences\n",
    "    Returns\n",
    "    -------\n",
    "    Sentence similarity : float\n",
    "        Non-negative number\n",
    "    \"\"\"\n",
    "    if (not s1) or (not s2):\n",
    "        return 0\n",
    "\n",
    "    s1 = Counter(s1)\n",
    "    s2 = Counter(s2)\n",
    "    norm1 = math.sqrt(sum(v ** 2 for v in s1.values()))\n",
    "    norm2 = math.sqrt(sum(v ** 2 for v in s2.values()))\n",
    "    prod = 0\n",
    "    for k, v in s1.items():\n",
    "        prod += v * s2.get(k, 0)\n",
    "    return prod / (norm1 * norm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "japanese-tiger",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def pagerank(x, df=0.85, max_iter=30, bias=None):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    x : scipy.sparse.csr_matrix\n",
    "        shape = (n vertex, n vertex)\n",
    "    df : float\n",
    "        Damping factor, 0 < df < 1\n",
    "    max_iter : int\n",
    "        Maximum number of iteration\n",
    "    bias : numpy.ndarray or None\n",
    "        If None, equal bias\n",
    "    Returns\n",
    "    -------\n",
    "    R : numpy.ndarray\n",
    "        PageRank vector. shape = (n vertex, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    assert 0 < df < 1\n",
    "\n",
    "    # initialize\n",
    "    A = normalize(x, axis=0, norm='l1')\n",
    "    R = np.ones(A.shape[0]).reshape(-1,1)\n",
    "\n",
    "    # check bias\n",
    "    if bias is None:\n",
    "        bias = (1 - df) * np.ones(A.shape[0]).reshape(-1,1)\n",
    "    else:\n",
    "        bias = bias.reshape(-1,1)\n",
    "        bias = A.shape[0] * bias / bias.sum()\n",
    "        assert bias.shape[0] == A.shape[0]\n",
    "        bias = (1 - df) * bias\n",
    "\n",
    "    # iteration\n",
    "    for _ in range(max_iter):\n",
    "        R = df * (A * R) + bias\n",
    "\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "sharp-brazilian",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class KeysentenceSummarizer:\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    sents : list of str\n",
    "        Sentence list\n",
    "    tokenize : callable\n",
    "        Tokenize function: tokenize(str) = list of str\n",
    "    min_count : int\n",
    "        Minumum frequency of words will be used to construct sentence graph\n",
    "    min_sim : float\n",
    "        Minimum similarity between sentences in sentence graph\n",
    "    similarity : str\n",
    "        available similarity = ['cosine', 'textrank']\n",
    "    vocab_to_idx : dict or None\n",
    "        Vocabulary to index mapper\n",
    "    df : float\n",
    "        PageRank damping factor\n",
    "    max_iter : int\n",
    "        Number of PageRank iterations\n",
    "    verbose : Boolean\n",
    "        If True, it shows training progress\n",
    "    \"\"\"\n",
    "    def __init__(self, sents=None, tokenize=None, min_count=2,\n",
    "        min_sim=0.3, similarity=None, vocab_to_idx=None,\n",
    "        df=0.85, max_iter=30, verbose=False):\n",
    "\n",
    "        self.tokenize = tokenize\n",
    "        self.min_count = min_count\n",
    "        self.min_sim = min_sim\n",
    "        self.similarity = similarity\n",
    "        self.vocab_to_idx = vocab_to_idx\n",
    "        self.df = df\n",
    "        self.max_iter = max_iter\n",
    "        self.verbose = verbose\n",
    "\n",
    "        if sents is not None:\n",
    "            self.train_textrank(sents)\n",
    "\n",
    "    def train_textrank(self, sents, bias=None):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        sents : list of str\n",
    "            Sentence list\n",
    "        bias : None or numpy.ndarray\n",
    "            PageRank bias term\n",
    "            Shape must be (n_sents,)\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        g = sent_graph(sents, self.tokenize, self.min_count,\n",
    "            self.min_sim, self.similarity, self.vocab_to_idx, self.verbose)\n",
    "        self.R = pagerank(g, self.df, self.max_iter, bias).reshape(-1)\n",
    "        if self.verbose:\n",
    "            print('trained TextRank. n sentences = {}'.format(self.R.shape[0]))\n",
    "\n",
    "    def summarize(self, sents, topk=30, bias=None):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        sents : list of str\n",
    "            Sentence list\n",
    "        topk : int\n",
    "            Number of key-sentences to be selected.\n",
    "        bias : None or numpy.ndarray\n",
    "            PageRank bias term\n",
    "            Shape must be (n_sents,)\n",
    "        Returns\n",
    "        -------\n",
    "        keysents : list of tuple\n",
    "            Each tuple stands for (sentence index, rank, sentence)\n",
    "        Usage\n",
    "        -----\n",
    "            >>> from textrank import KeysentenceSummarizer\n",
    "            >>> summarizer = KeysentenceSummarizer(tokenize = tokenizer, min_sim = 0.5)\n",
    "            >>> keysents = summarizer.summarize(texts, topk=30)\n",
    "        \"\"\"\n",
    "        n_sents = len(sents)\n",
    "        if isinstance(bias, np.ndarray):\n",
    "            if bias.shape != (n_sents,):\n",
    "                raise ValueError('The shape of bias must be (n_sents,) but {}'.format(bias.shape))\n",
    "        elif bias is not None:\n",
    "            raise ValueError('The type of bias must be None or numpy.ndarray but the type is {}'.format(type(bias)))\n",
    "\n",
    "        self.train_textrank(sents, bias)\n",
    "        idxs = self.R.argsort()[-topk:]\n",
    "        keysents = [(idx, self.R[idx], sents[idx]) for idx in reversed(idxs)]\n",
    "        return keysents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "cooked-smoke",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지난 13일부터 이날까지 최근 1주일간 하루 평균 신규 확진자는 약 651명꼴이다.\n",
      "19일 중앙방역대책본부(중대본)에 따르면 이날 0시 기준 코로나19 신규 확진자는 654명으로 전날(528명)보다 126명 늘었다.\n",
      "정부는 오는 21일 다음주부터 3주간 적용할 사회적 거리두기 조정안을 발표한다.\n"
     ]
    }
   ],
   "source": [
    "summarizer = KeysentenceSummarizer(\n",
    "    tokenize = lambda x:x.split(),\n",
    "    min_sim = 0.3,\n",
    "    verbose = False\n",
    ")\n",
    "keysents = summarizer.summarize(text_list, topk=3)\n",
    "for _, _, text_list in keysents:\n",
    "    print(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-police",
   "metadata": {},
   "source": [
    "기사마다 고유의 아이디가 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-cleaner",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleased-closer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme-canadian",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-engagement",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-terminal",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ambient-ceremony",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invisible-values",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-netherlands",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-commissioner",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organizational-contributor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-bradley",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
