{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coated-lambda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mediterranean-thinking",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-cameroon",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-celebrity",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worth-version",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "adapted-workplace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "15595 15595\n"
     ]
    }
   ],
   "source": [
    "with open('./lalaland_komaoran.txt', encoding='utf-8') as f:\n",
    "    sents = [sent.strip() for sent in f]\n",
    "\n",
    "with open('./lalaland.txt', encoding='utf-8') as f:\n",
    "    texts = [sent.strip() for sent in f]\n",
    "    \n",
    "    \n",
    "print(type(sents))\n",
    "print(type(texts))\n",
    "print(len(sents), len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "binary-swedish",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "controversial-elevation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "careful-tactics",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def scan_vocabulary(sents, tokenize=None, min_count=2):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    sents : list of str\n",
    "        Sentence list\n",
    "    tokenize : callable\n",
    "        tokenize(str) returns list of str\n",
    "    min_count : int\n",
    "        Minumum term frequency\n",
    "    Returns\n",
    "    -------\n",
    "    idx_to_vocab : list of str\n",
    "        Vocabulary list\n",
    "    vocab_to_idx : dict\n",
    "        Vocabulary to index mapper.\n",
    "    \"\"\"\n",
    "    counter = Counter(w for sent in sents for w in tokenize(sent))\n",
    "    counter = {w:c for w,c in counter.items() if c >= min_count}\n",
    "    idx_to_vocab = [w for w, _ in sorted(counter.items(), key=lambda x:-x[1])]\n",
    "    vocab_to_idx = {vocab:idx for idx, vocab in enumerate(idx_to_vocab)}\n",
    "    return idx_to_vocab, vocab_to_idx\n",
    "\n",
    "def tokenize_sents(sents, tokenize):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    sents : list of str\n",
    "        Sentence list\n",
    "    tokenize : callable\n",
    "        tokenize(sent) returns list of str (word sequence)\n",
    "    Returns\n",
    "    -------\n",
    "    tokenized sentence list : list of list of str\n",
    "    \"\"\"\n",
    "    return [tokenize(sent) for sent in sents]\n",
    "\n",
    "def vectorize(tokens, vocab_to_idx):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    tokens : list of list of str\n",
    "        Tokenzed sentence list\n",
    "    vocab_to_idx : dict\n",
    "        Vocabulary to index mapper\n",
    "    Returns\n",
    "    -------\n",
    "    sentence bow : scipy.sparse.csr_matrix\n",
    "        shape = (n_sents, n_terms)\n",
    "    \"\"\"\n",
    "    rows, cols, data = [], [], []\n",
    "    for i, tokens_i in enumerate(tokens):\n",
    "        for t, c in Counter(tokens_i).items():\n",
    "            j = vocab_to_idx.get(t, -1)\n",
    "            if j == -1:\n",
    "                continue\n",
    "            rows.append(i)\n",
    "            cols.append(j)\n",
    "            data.append(c)\n",
    "    n_sents = len(tokens)\n",
    "    n_terms = len(vocab_to_idx)\n",
    "    x = csr_matrix((data, (rows, cols)), shape=(n_sents, n_terms))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "multiple-intro",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def pagerank(x, df=0.85, max_iter=30, bias=None):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    x : scipy.sparse.csr_matrix\n",
    "        shape = (n vertex, n vertex)\n",
    "    df : float\n",
    "        Damping factor, 0 < df < 1\n",
    "    max_iter : int\n",
    "        Maximum number of iteration\n",
    "    bias : numpy.ndarray or None\n",
    "        If None, equal bias\n",
    "    Returns\n",
    "    -------\n",
    "    R : numpy.ndarray\n",
    "        PageRank vector. shape = (n vertex, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    assert 0 < df < 1\n",
    "\n",
    "    # initialize\n",
    "    A = normalize(x, axis=0, norm='l1')\n",
    "    R = np.ones(A.shape[0]).reshape(-1,1)\n",
    "\n",
    "    # check bias\n",
    "    if bias is None:\n",
    "        bias = (1 - df) * np.ones(A.shape[0]).reshape(-1,1)\n",
    "    else:\n",
    "        bias = bias.reshape(-1,1)\n",
    "        bias = A.shape[0] * bias / bias.sum()\n",
    "        assert bias.shape[0] == A.shape[0]\n",
    "        bias = (1 - df) * bias\n",
    "\n",
    "    # iteration\n",
    "    for _ in range(max_iter):\n",
    "        R = df * (A * R) + bias\n",
    "\n",
    "    return R\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "knowing-insurance",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def word_graph(sents, tokenize=None, min_count=2, window=2,\n",
    "    min_cooccurrence=2, vocab_to_idx=None, verbose=False):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    sents : list of str\n",
    "        Sentence list\n",
    "    tokenize : callable\n",
    "        tokenize(str) returns list of str\n",
    "    min_count : int\n",
    "        Minumum term frequency\n",
    "    window : int\n",
    "        Co-occurrence window size\n",
    "    min_cooccurrence : int\n",
    "        Minimum cooccurrence frequency\n",
    "    vocab_to_idx : dict\n",
    "        Vocabulary to index mapper.\n",
    "        If None, this function scan vocabulary first.\n",
    "    verbose : Boolean\n",
    "        If True, verbose mode on\n",
    "    Returns\n",
    "    -------\n",
    "    co-occurrence word graph : scipy.sparse.csr_matrix\n",
    "    idx_to_vocab : list of str\n",
    "        Word list corresponding row and column\n",
    "    \"\"\"\n",
    "    if vocab_to_idx is None:\n",
    "        idx_to_vocab, vocab_to_idx = scan_vocabulary(sents, tokenize, min_count)\n",
    "    else:\n",
    "        idx_to_vocab = [vocab for vocab, _ in sorted(vocab_to_idx.items(), key=lambda x:x[1])]\n",
    "\n",
    "    tokens = tokenize_sents(sents, tokenize)\n",
    "    g = cooccurrence(tokens, vocab_to_idx, window, min_cooccurrence, verbose)\n",
    "    return g, idx_to_vocab\n",
    "\n",
    "def cooccurrence(tokens, vocab_to_idx, window=2, min_cooccurrence=2, verbose=False):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    tokens : list of list of str\n",
    "        Tokenized sentence list\n",
    "    vocab_to_idx : dict\n",
    "        Vocabulary to index mapper\n",
    "    window : int\n",
    "        Co-occurrence window size\n",
    "    min_cooccurrence : int\n",
    "        Minimum cooccurrence frequency\n",
    "    verbose : Boolean\n",
    "        If True, verbose mode on\n",
    "    Returns\n",
    "    -------\n",
    "    co-occurrence matrix : scipy.sparse.csr_matrix\n",
    "        shape = (n_vocabs, n_vocabs)\n",
    "    \"\"\"\n",
    "    counter = defaultdict(int)\n",
    "    for s, tokens_i in enumerate(tokens):\n",
    "        if verbose and s % 1000 == 0:\n",
    "            print('\\rword cooccurrence counting {}'.format(s), end='')\n",
    "        vocabs = [vocab_to_idx[w] for w in tokens_i if w in vocab_to_idx]\n",
    "        n = len(vocabs)\n",
    "        for i, v in enumerate(vocabs):\n",
    "            if window <= 0:\n",
    "                b, e = 0, n\n",
    "            else:\n",
    "                b = max(0, i - window)\n",
    "                e = min(i + window, n)\n",
    "            for j in range(b, e):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                counter[(v, vocabs[j])] += 1\n",
    "                counter[(vocabs[j], v)] += 1\n",
    "    counter = {k:v for k,v in counter.items() if v >= min_cooccurrence}\n",
    "    n_vocabs = len(vocab_to_idx)\n",
    "    if verbose:\n",
    "        print('\\rword cooccurrence counting from {} sents was done'.format(s+1))\n",
    "    return dict_to_mat(counter, n_vocabs, n_vocabs)\n",
    "\n",
    "def dict_to_mat(d, n_rows, n_cols):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    d : dict\n",
    "        key : (i,j) tuple\n",
    "        value : float value\n",
    "    Returns\n",
    "    -------\n",
    "    scipy.sparse.csr_matrix\n",
    "    \"\"\"\n",
    "    rows, cols, data = [], [], []\n",
    "    for (i, j), v in d.items():\n",
    "        rows.append(i)\n",
    "        cols.append(j)\n",
    "        data.append(v)\n",
    "    return csr_matrix((data, (rows, cols)), shape=(n_rows, n_cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "occupied-parameter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class KeywordSummarizer:\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    sents : list of str\n",
    "        Sentence list\n",
    "    tokenize : callable\n",
    "        Tokenize function: tokenize(str) = list of str\n",
    "    min_count : int\n",
    "        Minumum frequency of words will be used to construct sentence graph\n",
    "    window : int\n",
    "        Word cooccurrence window size. Default is -1.\n",
    "        '-1' means there is cooccurrence between two words if the words occur in a sentence\n",
    "    min_cooccurrence : int\n",
    "        Minimum cooccurrence frequency of two words\n",
    "    vocab_to_idx : dict or None\n",
    "        Vocabulary to index mapper\n",
    "    df : float\n",
    "        PageRank damping factor\n",
    "    max_iter : int\n",
    "        Number of PageRank iterations\n",
    "    verbose : Boolean\n",
    "        If True, it shows training progress\n",
    "    \"\"\"\n",
    "    def __init__(self, sents=None, tokenize=None, min_count=2,\n",
    "        window=-1, min_cooccurrence=2, vocab_to_idx=None,\n",
    "        df=0.85, max_iter=30, verbose=False):\n",
    "\n",
    "        self.tokenize = tokenize\n",
    "        self.min_count = min_count\n",
    "        self.window = window\n",
    "        self.min_cooccurrence = min_cooccurrence\n",
    "        self.vocab_to_idx = vocab_to_idx\n",
    "        self.df = df\n",
    "        self.max_iter = max_iter\n",
    "        self.verbose = verbose\n",
    "\n",
    "        if sents is not None:\n",
    "            self.train_textrank(sents)\n",
    "\n",
    "    def train_textrank(self, sents, bias=None):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        sents : list of str\n",
    "            Sentence list\n",
    "        bias : None or numpy.ndarray\n",
    "            PageRank bias term\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "\n",
    "        g, self.idx_to_vocab = word_graph(sents,\n",
    "            self.tokenize, self.min_count,self.window,\n",
    "            self.min_cooccurrence, self.vocab_to_idx, self.verbose)\n",
    "        self.R = pagerank(g, self.df, self.max_iter, bias).reshape(-1)\n",
    "        if self.verbose:\n",
    "            print('trained TextRank. n words = {}'.format(self.R.shape[0]))\n",
    "\n",
    "    def keywords(self, topk=30):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        topk : int\n",
    "            Number of keywords selected from TextRank\n",
    "        Returns\n",
    "        -------\n",
    "        keywords : list of tuple\n",
    "            Each tuple stands for (word, rank)\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'R'):\n",
    "            raise RuntimeError('Train textrank first or use summarize function')\n",
    "        idxs = self.R.argsort()[-topk:]\n",
    "        keywords = [(self.idx_to_vocab[idx], self.R[idx]) for idx in reversed(idxs)]\n",
    "        return keywords\n",
    "\n",
    "    def summarize(self, sents, topk=30):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        sents : list of str\n",
    "            Sentence list\n",
    "        topk : int\n",
    "            Number of keywords selected from TextRank\n",
    "        Returns\n",
    "        -------\n",
    "        keywords : list of tuple\n",
    "            Each tuple stands for (word, rank)\n",
    "        \"\"\"\n",
    "        self.train_textrank(sents)\n",
    "        return self.keywords(topk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "challenging-reference",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 0)) while a minimum of 1 is required by the normalize function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-9970b672e2ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m )\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mkeywords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeyword_extractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{} ({:.3})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-94a3ff0afc06>\u001b[0m in \u001b[0;36msummarize\u001b[0;34m(self, sents, topk)\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mEach\u001b[0m \u001b[0mtuple\u001b[0m \u001b[0mstands\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_textrank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-94a3ff0afc06>\u001b[0m in \u001b[0;36mtrain_textrank\u001b[0;34m(self, sents, bias)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             self.min_cooccurrence, self.vocab_to_idx, self.verbose)\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpagerank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'trained TextRank. n words = {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-f8b762c9d7a2>\u001b[0m in \u001b[0;36mpagerank\u001b[0;34m(x, df, max_iter, bias)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# initialize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'l1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(X, norm, axis, copy, return_norm)\u001b[0m\n\u001b[1;32m   1902\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'%d' is not a supported axis\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1904\u001b[0;31m     X = check_array(X, accept_sparse=sparse_format, copy=copy,\n\u001b[0m\u001b[1;32m   1905\u001b[0m                     estimator='the normalize function', dtype=FLOAT_DTYPES)\n\u001b[1;32m   1906\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m             raise ValueError(\"Found array with %d sample(s) (shape=%s) while a\"\n\u001b[0m\u001b[1;32m    670\u001b[0m                              \u001b[0;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                              % (n_samples, array.shape, ensure_min_samples,\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 0)) while a minimum of 1 is required by the normalize function."
     ]
    }
   ],
   "source": [
    "def komoran_tokenize(sent):\n",
    "    words = sent.split()\n",
    "    words = [w for w in words if ('/NN' in w or '/XR' in w or '/VA' in w or '/VV' in w)]\n",
    "    return words\n",
    "\n",
    "keyword_extractor = KeywordSummarizer(\n",
    "    tokenize = komoran_tokenize,\n",
    "    window = -1,\n",
    "    verbose = False\n",
    ")\n",
    "keywords = keyword_extractor.summarize(news1, topk=30)\n",
    "for word, rank in keywords:\n",
    "    print('{} ({:.3})'.format(word, rank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-binding",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-major",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
