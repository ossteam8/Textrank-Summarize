{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "accessible-needle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: cp949 -*- \n",
    "import platform\n",
    "from collections import Counter\n",
    "import kss\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "if platform.system() == \"Windows\":\n",
    "    try:\n",
    "        from eunjeon import Mecab\n",
    "    except:\n",
    "        print(\"please install eunjeon module\")\n",
    "else:  # Ubuntu일 경우\n",
    "    from konlpy.tag import Mecab\n",
    "\n",
    "from typing import List, Callable, Union, Any, TypeVar, Tuple, Dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "liberal-skiing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(tokenizer_name):\n",
    "    tokenizer = Mecab()\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "fourth-globe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(sent: List[str], noun=False, tokenizer=\"mecab\") -> List[str]:\n",
    "    tokenizer = get_tokenizer(tokenizer)\n",
    "    \n",
    "    if noun:\n",
    "        nouns = tokenizer.nouns(sent)\n",
    "        nouns = [word for word in nouns if len(word) > 1]\n",
    "        return nouns\n",
    "\n",
    "    return tokenizer.morphs(sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "pregnant-building",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def vectorize_sents(\n",
    "    sents: List[str],\n",
    "    stopwords=None,\n",
    "    min_count=2,\n",
    "    tokenizer=\"mecab\",\n",
    "    noun=False\n",
    "):\n",
    "\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        stop_words=stopwords,\n",
    "        tokenizer=partial(get_tokens, noun=noun, tokenizer=\"mecab\"),\n",
    "        min_df=min_count,\n",
    "    )\n",
    "\n",
    "    vec = vectorizer.fit_transform(sents)\n",
    "    vocab_idx = vectorizer.vocabulary_\n",
    "    idx_vocab = {idx: vocab for vocab, idx in vocab_idx.items()}\n",
    "    return vec, vocab_idx, idx_vocab\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "liable-scholar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_matrix(x, min_sim=0.3, min_length=1):\n",
    "\n",
    "    # binary csr_matrix\n",
    "    numerators = (x > 0) * 1\n",
    "\n",
    "    #문장간 유사도 계산, 문장간 유사도가 0.3이하면 간선 연결하지 않음.\n",
    "    min_length = 1\n",
    "    denominators = np.asarray(x.sum(axis=1))\n",
    "    denominators[np.where(denominators <= min_length)] = 10000\n",
    "    denominators = np.log(denominators)\n",
    "    denom_log1 = np.matmul(denominators, np.ones(denominators.shape).T)\n",
    "    denom_log2 = np.matmul(np.ones(denominators.shape), denominators.T)\n",
    "\n",
    "    sim_mat = np.dot(numerators, numerators.T)\n",
    "    sim_mat = sim_mat / (denom_log1 + denom_log2)\n",
    "    sim_mat[np.where(sim_mat <= min_sim)] = 0\n",
    "\n",
    "    return sim_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-advice",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "active-receipt",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sent_graph(\n",
    "    sents: List[str],\n",
    "    min_count=2,\n",
    "    min_sim=0.3,\n",
    "    tokenizer=\"mecab\",\n",
    "    noun=False,\n",
    "    stopwords: List[str] = [\"연합뉴스\", \"중앙일보\",\"한겨레\",\"국민일보\",\"머니투데이\",\"동아일보\"]\n",
    "):\n",
    "    \n",
    "    # TF-IDF + Cosine similarity \n",
    "\n",
    "    mat, vocab_idx, idx_vocab = vectorize_sents(\n",
    "        sents, stopwords, min_count=min_count, tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    \n",
    "    mat = similarity_matrix(mat, min_sim=min_sim)\n",
    "\n",
    "    return mat, vocab_idx, idx_vocab\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "prospective-vegetable",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "여기에 사건에 대한 진실 규명을 요구하는 단체가 경찰과 A씨의 휴대전화를 발견한 환경미화원을 검찰에 고발하면서 이른바 ‘한강 대학생 사건’이 고소·고발전으로 번지고 있다.\n",
      "A씨 측 반격 \"수만명 고소해야\" 손씨의 사망 원인을 밝히기 위한 경찰 수사가 마무리 단계에 접어들었지만 고소·고발에 집회까지 이어지면서 혼란이 가중되고 있다.\n",
      "A씨를 대리하는 정 변호사의 고소 대상에는 유튜버와 블로거뿐 아니라 인터넷 커뮤니티 등에 게시글이나 댓글을 작성한 이들도 포함될 예정이다.\n"
     ]
    }
   ],
   "source": [
    "news3 = \"\"\"한강에서 사망한 채 발견된 손정민씨의 실종 당일 함께 술을 마셨던 A씨 측이 대규모 고소를 예고했다. 여기에 사건에 대한 진실 규명을 요구하는 단체가 경찰과 A씨의 휴대전화를 발견한 환경미화원을 검찰에 고발하면서 이른바 ‘한강 대학생 사건’이 고소·고발전으로 번지고 있다. 해당 단체는 추가 고발까지 예고했다.A씨 측 반격 \"수만명 고소해야\" 손씨의 사망 원인을 밝히기 위한 경찰 수사가 마무리 단계에 접어들었지만 고소·고발에 집회까지 이어지면서 혼란이 가중되고 있다. 5일 A씨 측 등에 따르면 정병원 변호사(법무법인 원앤파트너스)는 A씨와 그 가족 등에 대한 허위사실을 제기한 유튜버와 블로거 등을 7일부터 경찰에 고소할 예정이다.A씨를 대리하는 정 변호사의 고소 대상에는 유튜버와 블로거뿐 아니라 인터넷 커뮤니티 등에 게시글이나 댓글을 작성한 이들도 포함될 예정이다. 정 변호사는 “수차례 친구 A 및 그 가족과 주변인들에 관한 위법행위를 멈춰달라고 요청했음에도 게시물이 오히려 늘어나고 있다”며 “일부 내용은 수인한도를 넘어서면서 피해와 고통은 점점 더 심해지고 있다”고 밝혔다. 이어 “선처를 희망하는 사람이 전혀 없다면 최소 수만 명은 고소해야 할 것 같다”고 덧붙였다.이들은 지난달 29일에도 집회를 열고 손씨의 실종 당일 목격자의 제보를 독려하는 집회를 열어다. 지난 1일에는 서울 종로구 서울경찰청 앞에서 기자회견을 열고 “수사당국이 경찰 초동수사 부실 논란과 손씨 사망 경위에 대한 의혹을 피해왔다”고 주장했다. 반진사를 만든 건 유튜브 ‘종이의 TV’ 채널 운영자다. A씨 측이 명예훼손 등 혐의로 고소하겠다고 밝힌 유튜버 중엔 '종이의 TV‘도 포함돼 있다.손정민 친구 \"수만명 고소\"에 \"끝까지 간다\"…혼란의 한강 사망\"\"\"\n",
    "#즉, 입력값 str 형태\n",
    "\n",
    "sents= kss.split_sentences(news3)\n",
    "mat, vocab_idx, idx_vocab = sent_graph(sents)\n",
    "R = pagerank(mat)\n",
    "topk = 3\n",
    "idxs = R.argsort()[-topk:]\n",
    "#keysents = [(idx, R[idx], sents[idx]) for idx in sorted(idxs)]\n",
    "for idx in sorted(idxs):\n",
    "    print(sents[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abroad-cameroon",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mental-soccer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-integration",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "hindu-celebrity",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "proud-cisco",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "affiliated-morocco",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mexican-consent",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "instrumental-suggestion",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "reserved-reading",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "married-poultry",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "major-trigger",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endangered-gardening",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "swedish-cylinder",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "prostate-forest",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "falling-kingston",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relevant-influence",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "material-ribbon",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ideal-country",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphic-appearance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experimental-subsection",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
