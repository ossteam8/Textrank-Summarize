{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "french-floor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def scan_vocabulary(sents, tokenize, min_count=2):\n",
    "    counter = Counter(w for sent in sents for w in tokenize(sent))\n",
    "    counter = {w:c for w,c in counter.items() if c >= min_count}\n",
    "    idx_to_vocab = [w for w, _ in sorted(counter.items(), key=lambda x:-x[1])]\n",
    "    vocab_to_idx = {vocab:idx for idx, vocab in enumerate(idx_to_vocab)}\n",
    "    return idx_to_vocab, vocab_to_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "through-hierarchy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def cooccurrence(tokens, vocab_to_idx, window=2, min_cooccurrence=2):\n",
    "    counter = defaultdict(int)\n",
    "    for s, tokens_i in enumerate(tokens):\n",
    "        vocabs = [vocab_to_idx[w] for w in tokens_i if w in vocab_to_idx]\n",
    "        n = len(vocabs)\n",
    "        for i, v in enumerate(vocabs):\n",
    "            if window <= 0:\n",
    "                b, e = 0, n\n",
    "            else:\n",
    "                b = max(0, i - window)\n",
    "                e = min(i + window, n)\n",
    "            for j in range(b, e):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                counter[(v, vocabs[j])] += 1\n",
    "                counter[(vocabs[j], v)] += 1\n",
    "    counter = {k:v for k,v in counter.items() if v >= min_cooccurrence}\n",
    "    n_vocabs = len(vocab_to_idx)\n",
    "    return dict_to_mat(counter, n_vocabs, n_vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "latin-casino",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def dict_to_mat(d, n_rows, n_cols):\n",
    "    rows, cols, data = [], [], []\n",
    "    for (i, j), v in d.items():\n",
    "        rows.append(i)\n",
    "        cols.append(j)\n",
    "        data.append(v)\n",
    "    return csr_matrix((data, (rows, cols)), shape=(n_rows, n_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "consecutive-twenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_graph(sents, tokenize=None, min_count=2, window=2, min_cooccurrence=2):\n",
    "    idx_to_vocab, vocab_to_idx = scan_vocabulary(sents, tokenize, min_count)\n",
    "    tokens = [tokenize(sent) for sent in sents]\n",
    "    g = cooccurrence(tokens, vocab_to_idx, window, min_cooccurrence, verbose)\n",
    "    return g, idx_to_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "simple-strengthening",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def pagerank(x, df=0.85, max_iter=30):\n",
    "    assert 0 < df < 1\n",
    "\n",
    "    # initialize\n",
    "    A = normalize(x, axis=0, norm='l1')\n",
    "    R = np.ones(A.shape[0]).reshape(-1,1)\n",
    "    bias = (1 - df) * np.ones(A.shape[0]).reshape(-1,1)\n",
    "\n",
    "    # iteration\n",
    "    for _ in range(max_iter):\n",
    "        R = df * (A * R) + bias\n",
    "\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "foster-blank",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textrank_keyword(sents, tokenize, min_count, window, min_cooccurrence, df=0.85, max_iter=30, topk=30):\n",
    "    g, idx_to_vocab = word_graph(sents, tokenize, min_count, window, min_cooccurrence)\n",
    "    R = pagerank(g, df, max_iter).reshape(-1)\n",
    "    idxs = R.argsort()[-topk:]\n",
    "    keywords = [(idx_to_vocab[idx], R[idx]) for idx in reversed(idxs)]\n",
    "    return keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "personalized-component",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "\n",
    "def sent_graph(sents, tokenize, similarity, min_count=2, min_sim=0.3):\n",
    "    _, vocab_to_idx = scan_vocabulary(sents, tokenize, min_count)\n",
    "\n",
    "    tokens = [[w for w in tokenize(sent) if w in vocab_to_idx] for sent in sents]\n",
    "    rows, cols, data = [], [], []\n",
    "    n_sents = len(tokens)\n",
    "    for i, tokens_i in enumerate(tokens):\n",
    "        for j, tokens_j in enumerate(tokens):\n",
    "            if i >= j:\n",
    "                continue\n",
    "            sim = similarity(tokens_i, tokens_j)\n",
    "            if sim < min_sim:\n",
    "                continue\n",
    "            rows.append(i)\n",
    "            cols.append(j)\n",
    "            data.append(sim)\n",
    "    return csr_matrix((data, (rows, cols)), shape=(n_sents, n_sents))\n",
    "\n",
    "def textrank_sent_sim(s1, s2):\n",
    "    n1 = len(s1)\n",
    "    n2 = len(s2)\n",
    "    if (n1 <= 1) or (n2 <= 1):\n",
    "        return 0\n",
    "    common = len(set(s1).intersection(set(s2)))\n",
    "    base = math.log(n1) + math.log(n2)\n",
    "    return common / base\n",
    "\n",
    "def cosine_sent_sim(s1, s2):\n",
    "    if (not s1) or (not s2):\n",
    "        return 0\n",
    "\n",
    "    s1 = Counter(s1)\n",
    "    s2 = Counter(s2)\n",
    "    norm1 = math.sqrt(sum(v ** 2 for v in s1.values()))\n",
    "    norm2 = math.sqrt(sum(v ** 2 for v in s2.values()))\n",
    "    prod = 0\n",
    "    for k, v in s1.items():\n",
    "        prod += v * s2.get(k, 0)\n",
    "    return prod / (norm1 * norm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "killing-return",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-19-42ed95250708>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-19-42ed95250708>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    def textrank_keysentence(sents, tokenize, min_count, similarity, df=0.85, max_iter=30, topk=5 )\u001b[0m\n\u001b[0m                                                                                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def textrank_keysentence(sents, tokenize, min_count, similarity, df=0.85, max_iter=30, topk=5 )\n",
    "    g = sent_graph(sents, tokenize, min_count, min_sim, similarity)\n",
    "    R = pagerank(g, df, max_iter).reshape(-1)\n",
    "    idxs = R.argsort()[-topk:]\n",
    "    keysents = [(idx, R[idx], sents[idx]) for idx in reversed(idxs)]\n",
    "    return keysents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "difficult-adams",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Komoran\n",
    "\n",
    "komoran = Komoran()\n",
    "def komoran_tokenize(sent):\n",
    "    words = komoran.pos(sent, join=True)\n",
    "    words = [w for w in words if ('/NN' in w or '/XR' in w or '/VA' in w or '/VV' in w)]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "grave-checkout",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'textrank'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-d37675328116>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtextrank\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKeywordSummarizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m keyword_extractor = KeywordSummarizer(\n\u001b[1;32m      4\u001b[0m     \u001b[0mtokenize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkomoran_tokenize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'textrank'"
     ]
    }
   ],
   "source": [
    "from textrank import KeywordSummarizer\n",
    "\n",
    "keyword_extractor = KeywordSummarizer(\n",
    "    tokenize = komoran_tokenize,\n",
    "    window = -1,\n",
    "    verbose = False\n",
    ")\n",
    "\n",
    "keywords = keyword_extractor.summarize(sents, topk=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experienced-sociology",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
